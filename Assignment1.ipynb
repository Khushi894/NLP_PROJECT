{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm8YzXFUJ/20yWJIbF3+Er",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khushi894/NLP_PROJECT/blob/main/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d77e859"
      },
      "source": [
        "### Custom Paragraph for Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a421a7c"
      },
      "source": [
        "custom_paragraph = \"\"\"Text preprocessing is a vital step in natural language processing (NLP) tasks. It involves cleaning and transforming raw text into a more suitable format for machine learning models. Common steps include tokenization, removing stopwords, stemming, and lemmatization. These techniques help reduce noise and improve the efficiency and accuracy of NLP models.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbfd9655"
      },
      "source": [
        "### Step 1: Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f64cabe9",
        "outputId": "abe9acb6-33e4-4d6c-e5d9-e43f1ac350eb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Adding this download for the 'punkt_tab' resource\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(custom_paragraph)\n",
        "print(\"Tokenized Text:\", tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Text: ['Text', 'preprocessing', 'is', 'a', 'vital', 'step', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', 'tasks', '.', 'It', 'involves', 'cleaning', 'and', 'transforming', 'raw', 'text', 'into', 'a', 'more', 'suitable', 'format', 'for', 'machine', 'learning', 'models', '.', 'Common', 'steps', 'include', 'tokenization', ',', 'removing', 'stopwords', ',', 'stemming', ',', 'and', 'lemmatization', '.', 'These', 'techniques', 'help', 'reduce', 'noise', 'and', 'improve', 'the', 'efficiency', 'and', 'accuracy', 'of', 'NLP', 'models', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "445d1f54"
      },
      "source": [
        "### Step 2: Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "844eca2f",
        "outputId": "4a8f60a8-b6db-44b4-9d0f-de64ff52d1d5"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(\"After Stopword Removal:\", filtered_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Stopword Removal: ['Text', 'preprocessing', 'vital', 'step', 'natural', 'language', 'processing', '(', 'NLP', ')', 'tasks', '.', 'involves', 'cleaning', 'transforming', 'raw', 'text', 'suitable', 'format', 'machine', 'learning', 'models', '.', 'Common', 'steps', 'include', 'tokenization', ',', 'removing', 'stopwords', ',', 'stemming', ',', 'lemmatization', '.', 'techniques', 'help', 'reduce', 'noise', 'improve', 'efficiency', 'accuracy', 'NLP', 'models', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c45f83a6"
      },
      "source": [
        "### Step 3: Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c992a1a",
        "outputId": "f676aae4-97d9-46fb-9c4d-86a4ce8b2314"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(\"After Stemming:\", stemmed_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Stemming: ['text', 'preprocess', 'vital', 'step', 'natur', 'languag', 'process', '(', 'nlp', ')', 'task', '.', 'involv', 'clean', 'transform', 'raw', 'text', 'suitabl', 'format', 'machin', 'learn', 'model', '.', 'common', 'step', 'includ', 'token', ',', 'remov', 'stopword', ',', 'stem', ',', 'lemmat', '.', 'techniqu', 'help', 'reduc', 'nois', 'improv', 'effici', 'accuraci', 'nlp', 'model', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0015b4f"
      },
      "source": [
        "### Step 4: Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40c51ba4",
        "outputId": "eb20cf33-a1b2-4ea8-f7e8-7feafd7ceef6"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print(\"After Lemmatization:\", lemmatized_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Lemmatization: ['Text', 'preprocessing', 'vital', 'step', 'natural', 'language', 'processing', '(', 'NLP', ')', 'task', '.', 'involves', 'cleaning', 'transforming', 'raw', 'text', 'suitable', 'format', 'machine', 'learning', 'model', '.', 'Common', 'step', 'include', 'tokenization', ',', 'removing', 'stopwords', ',', 'stemming', ',', 'lemmatization', '.', 'technique', 'help', 'reduce', 'noise', 'improve', 'efficiency', 'accuracy', 'NLP', 'model', '.']\n"
          ]
        }
      ]
    }
  ]
}